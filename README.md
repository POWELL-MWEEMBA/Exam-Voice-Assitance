
# Exam-Voice-Assitance
# ExamAssistant - Smart AI Exam System for Visually Impaired Users

A comprehensive React Native mobile application designed to provide an accessible exam-taking experience for visually impaired students, with advanced features including voice navigation, speech recognition, offline support, and real-time analytics.

## ğŸ¯ Project Overview

ExamAssistant is a full-stack mobile application built with React Native and Expo that enables:

- **Examiners** to create, manage, and analyze exams
- **Students** (especially visually impaired) to take exams independently using voice commands and accessibility features
- **Research analytics** for evaluating the effectiveness of accessibility features

This project addresses real-world accessibility challenges in educational technology, specifically designed for research.

## ğŸ§  Problem Statement

Traditional exam-taking platforms rely heavily on visual interfaces, creating significant barriers for visually impaired students. These students typically require human scribes or assistive technologies that are expensive, unreliable, or unavailable. Current solutions often:

- Require visual navigation (buttons, menus, text fields)
- Depend on external assistance (human scribes)
- Lack real-time feedback and independent operation
- Fail to provide comprehensive analytics for accessibility research

ExamAssistant addresses this accessibility gap by providing a **fully voice-driven, independent exam-taking experience** that enables visually impaired students to complete exams autonomously without visual dependencies or external assistance.

## ğŸš€ Key Features

### Voice-Driven Navigation

- Complete exam interaction through voice commands
- No visual UI required for core functionality
- Screen reader compatible architecture

### Simple Command Set

- Intuitive voice commands (e.g., "Read question", "Select option A", "Submit answer")
- Context-aware command processing (commands only work in appropriate screens)
- Reduced cognitive load with minimal command vocabulary

### No Visual Dependency

- Questions and options read aloud via Text-to-Speech (TTS)
- Answers provided via Speech-to-Text (STT)
- Haptic feedback for confirmations and navigation
- Fully accessible without screen interaction

### For Students

- âœ… **Accessibility-First Design**: Voice navigation, speech recognition, and haptic feedback
- âœ… **Offline Support**: Take exams without internet connection with automatic sync when online
- âœ… **Auto-Save**: Answers are automatically saved locally to prevent data loss
- âœ… **Multiple Question Types**: Support for multiple choice and written answer questions
- âœ… **Real-Time Results**: Instant grading and results viewing
- âœ… **Usability Surveys**: Post-exam feedback collection for research

### For Examiners

- âœ… **Exam Creation**: Create exams with multiple question types
- âœ… **Exam Management**: Edit, activate, and manage exam status
- âœ… **Submissions Tracking**: View all student submissions
- âœ… **Research Analytics**: Comprehensive analytics dashboard with:
  - Usage metrics (voice commands, read-aloud requests)
  - Completion rates
  - Feature usage statistics
  - SUS (System Usability Scale) scores

### Technical Highlights

- âœ… **Role-Based Authentication**: Secure Firebase Authentication with role management
- âœ… **Real-Time Database**: Firestore for scalable data storage
- âœ… **Offline-First Architecture**: AsyncStorage for local caching and sync
- âœ… **Context API**: Global state management for authentication and accessibility
- âœ… **Navigation System**: React Navigation with role-based routing
- âœ… **Evaluation Metrics**: Comprehensive tracking system for research purposes

## ğŸ”„ Interaction Flow (CRITICAL)

The system enforces a **strict turn-based interaction model** where TTS completes before STT activates, preventing feedback loops and ensuring reliable voice interaction.

### State Management

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   IDLE      â”‚ â† Initial state
â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
      â”‚
      â”œâ”€â”€â”€ User Action â”€â”€â”€â”
      â”‚                    â”‚
      â–¼                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ TTS Speaking â”‚    â”‚ STT Listeningâ”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚                    â”‚
       â”‚                    â”‚
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â”‚
                â–¼
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚   Process    â”‚
         â”‚   Command    â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Detailed Flow:

1. **TTS Activation** (System Output):

   - User triggers "Read question" command
   - System **stops any active STT** (`stopListening()`)
   - **Waits 500ms** to ensure recognition fully stops
   - Sets `isSpeaking = true` (locks STT)
   - Speaks question text
   - **Waits 300ms** after completion (buffer to prevent feedback)
   - Sets `isSpeaking = false` and calls `onSpeechEndCallback()`

2. **STT Activation** (User Input):

   - User says voice command
   - System **stops any active TTS** (`stopSpeaking()`)
   - **Waits 800ms** to ensure audio fully clears
   - Sets `isListening = true` (locks TTS)
   - Processes recognition result
   - Sets `isListening = false` after processing

3. **State Locking Prevention**:
   - `isSpeaking` and `isListening` are **mutually exclusive**
   - Cannot be `true` simultaneously
   - Prevents microphone from picking up TTS output
   - Prevents TTS from interrupting user commands

### Example Sequence:

```
1. User: "Read question"
   â†’ STT stops â†’ [500ms wait] â†’ TTS starts â†’ "Question 1: What is..."
   â†’ [300ms wait] â†’ STT can resume

2. User: "Select option A"
   â†’ TTS stops â†’ [800ms wait] â†’ STT starts â†’ Processes command
   â†’ Validates context â†’ Executes action â†’ STT stops

3. System: "Selected option A"
   â†’ [500ms wait] â†’ TTS confirms â†’ [300ms wait] â†’ Ready for next command
```

**Key Implementation**: The `SpeechService` class uses explicit state flags (`isSpeaking`, `isListening`) with enforced delays between transitions, ensuring the microphone never captures TTS output and TTS never interrupts user speech.

## âš ï¸ Challenges & Solutions

### Challenge 1: TTS and STT Triggering Each Other (Feedback Loop)

**Problem**: When TTS speaks, the microphone could pick up the audio output, causing STT to process system speech as user input, creating an infinite loop.

**Solution**: **Explicit State Locking with Timing Delays**

- Implemented mutual exclusion flags: `isSpeaking` and `isListening` cannot both be `true`
- Added **500ms delay** after stopping STT before starting TTS
- Added **800ms delay** after stopping TTS before starting STT (longer to clear audio buffer)
- Added **300ms buffer** after TTS completion before re-enabling STT
- State transitions are atomic and sequential

```javascript
// From speechService.js
if (this.isListening) {
  await this.stopListening();
  await new Promise((resolve) => setTimeout(resolve, 500)); // Critical delay
}
this.isSpeaking = true; // Locks STT
```

**Result**: Zero feedback loops in testing. System reliably distinguishes between user input and system output.

---

### Challenge 2: Misheard Commands or Ambiguous Recognition

**Problem**: Speech recognition may misinterpret commands, especially in noisy environments or with different accents, leading to incorrect actions.

**Solution**: **Command Confirmation Logic + Context Validation**

- Implemented screen context isolation (`ScreenContextService`) - commands only work in their designated screens
- Command validation checks context before execution
- Haptic feedback confirms actions (vibration on selection)
- Users can repeat commands if misheard
- Logging system tracks recognition accuracy for research

**Future Enhancement**: Command confirmation prompts ("Did you say 'select option A'?") for critical actions.

---

### Challenge 3: Screen Context Isolation

**Problem**: Voice commands from one screen could trigger actions on another screen (e.g., "Start exam" command triggering while viewing results).

**Solution**: **Screen Context Service**

- Each screen registers its context (`HOME`, `EXAM`, etc.)
- Commands are validated against current context before execution
- Context changes automatically when navigating between screens
- Prevents cross-screen command interference

```javascript
// From screenContextService.js
isCommandValid(command, context) {
  if (this.currentScreenContext !== context) {
    return false; // Command ignored if wrong context
  }
  return true;
}
```

---

### Challenge 4: Offline Functionality Without Native Network Modules

**Problem**: Need offline support but React Native's NetInfo requires native modules, which complicates setup.

**Solution**: **Fetch-Based Network Detection**

- Used `fetch()` with timeout to check connectivity (polling Google's 204 endpoint)
- Local caching with AsyncStorage for exams and answers
- Queue system for submissions (syncs when online)
- No native dependencies required

**Result**: Works fully offline, syncs automatically when connection restored.

---

### Challenge 5: Real-Time State Synchronization

**Problem**: Maintaining consistent state across multiple screens (authentication, accessibility settings, exam progress) without prop drilling.

**Solution**: **React Context API with Persistent Storage**

- `AuthContext` for authentication state
- `AccessibilityContext` for accessibility settings (persisted to AsyncStorage)
- Singleton services for cross-cutting concerns (SpeechService, ScreenContextService)
- Automatic state persistence and restoration

## ğŸ¤– Use of AI in Development

This project leveraged AI tools strategically to accelerate development while maintaining full code ownership and understanding.

### AI Tools Used:

- **ChatGPT/Claude**: Architecture planning, code review, debugging assistance
- **GitHub Copilot**: Code completion and boilerplate generation

### What Was Accepted:

âœ… **Architecture Decisions**: Used AI to explore accessibility patterns and best practices for TTS/STT coordination  
âœ… **Code Structure**: Adopted suggestions for service layer pattern and context management  
âœ… **Documentation**: AI-assisted in writing comprehensive comments and README sections  
âœ… **Bug Fixes**: Used AI to identify edge cases in state management (e.g., TTS/STT overlap)

### What Was Changed:

âš ï¸ **State Management Logic**: AI suggested simpler approaches, but implemented custom state locking with explicit delays for reliability  
âš ï¸ **Command Processing**: AI recommended regex-based parsing, but switched to context-aware command matching for better UX  
âš ï¸ **Error Handling**: Enhanced AI suggestions with comprehensive logging and user feedback

### What Was Rejected:

âŒ **Fully AI-Generated Components**: All code was written manually with AI as a reference  
âŒ **Complex NLP Libraries**: Rejected heavyweight NLP solutions in favor of simple command matching for performance  
âŒ **Auto-Generated Tests**: Wrote manual tests focusing on accessibility edge cases

### AI Fluency, Not Dependency:

- Understood every line of code and can explain architectural decisions
- Modified and debugged AI-suggested code independently
- Used AI as a productivity tool, not a crutch
- Maintained full control over codebase quality and performance

**Key Insight**: AI helped explore solutions faster, but all critical decisions (TTS/STT timing, state management, accessibility patterns) were validated through manual testing and user feedback.

## ğŸ“¥ Download & Test the App

### Quick Test with Expo Go

The easiest way to test the app is using Expo Go:

1. **Install Expo Go** on your device:

   - [Android - Google Play Store](https://play.google.com/store/apps/details?id=host.exp.exponent)
     Live link to the app: https://expo.dev/artifacts/eas/fHx7RRmHDj7PfsFH9QppaF.apk

2. **Run the development server:**

   ```bash
   npm start
   ```

3. **Scan the QR code** with:
   - **Android**: Expo Go app or Camera app
   - **iOS**: Camera app (will open in Expo Go)

### Build Standalone APK/IPA

To build a standalone app for testing:

```bash
# Install EAS CLI
npm install -g eas-cli

# Login to Expo
eas login

# Build for Android
eas build --platform android

# Build for iOS (requires Apple Developer account)
eas build --platform ios
```

### Download Pre-built APK

To get a shareable download link:

1. **Build the APK:**

   ```bash
   eas build --platform android --profile preview
   ```

2. **Get the download link:**
   - After the build completes, EAS will provide a download link
   - Share this link with recruiters/testers
   - Or visit: https://expo.dev/artifacts/eas/fHx7RRmHDj7PfsFH9QppaF.apk`

ğŸ“± **Alternative**: You can also build locally:

```bash
# Build APK locally (requires Android Studio)
npm run android -- --variant=release
```

The APK will be located at: `android/app/build/outputs/apk/release/app-release.apk`

> **Note**: Replace `[your-account]` with your Expo account username. For a direct download link, build the APK and host it on a file sharing service or your own server.

### Test Credentials (Demo Accounts)

For testing purposes, you can use these demo accounts:

**Examiner Account:**

- Email: `examiner@demo.com`
- Password: `demo123`

**Student Account:**

- Email: `student@demo.com`
- Password: `demo123`

> âš ï¸ **Important**: These are demo accounts. For production testing, create your own accounts through the registration screen.

### Testing Checklist

When testing the app, consider:

- âœ… **Authentication**: Login/Register functionality
- âœ… **Role-Based Access**: Different screens for examiners vs students
- âœ… **Exam Creation**: Create exams with multiple question types
- âœ… **Accessibility Features**: Voice commands, speech recognition
- âœ… **Offline Mode**: Test without internet connection
- âœ… **Auto-Save**: Verify answers are saved locally
- âœ… **Analytics**: Check research metrics dashboard

## ğŸ› ï¸ Technologies Used

### Frontend

- **React Native** (0.81.5) - Cross-platform mobile development
- **Expo** (^54.0.23) - Development platform and tooling
- **React Navigation** (^7.1.17) - Navigation library
- **React Context API** - State management

### Backend & Services

- **Firebase Authentication** - User authentication and authorization
- **Cloud Firestore** - NoSQL database for exams, users, and analytics
- **Firebase Storage** - File storage (configured)

### Accessibility & Media

- **expo-speech** (^14.0.8) - Text-to-speech functionality
- **expo-speech-recognition** (^3.0.1) - Speech-to-text for voice commands
- **@react-native-community/slider** - Accessible UI components

### Storage & Utilities

- **@react-native-async-storage/async-storage** (2.2.0) - Local data persistence
- **React Native Gesture Handler** - Touch gesture handling
- **React Native Safe Area Context** - Safe area handling

## ğŸ“ Project Structure

```
ExamAssistant/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ config/
â”‚   â”‚   â””â”€â”€ firebase.js              # Firebase configuration
â”‚   â”œâ”€â”€ constants/
â”‚   â”‚   â”œâ”€â”€ AppColors.js             # Color scheme
â”‚   â”‚   â”œâ”€â”€ GlobalStyles.js          # Global styling
â”‚   â”‚   â””â”€â”€ Spacing.js                # Spacing constants
â”‚   â”œâ”€â”€ context/
â”‚   â”‚   â”œâ”€â”€ AuthContext.js           # Authentication state management
â”‚   â”‚   â””â”€â”€ AccessibilityContext.js  # Accessibility settings
â”‚   â”œâ”€â”€ hooks/
â”‚   â”‚   â””â”€â”€ index.js                 # Custom React hooks
â”‚   â”œâ”€â”€ navigation/
â”‚   â”‚   â”œâ”€â”€ AppNavigator.js          # Main navigation controller
â”‚   â”‚   â”œâ”€â”€ AuthNav.js               # Authentication navigation
â”‚   â”‚   â”œâ”€â”€ examinerNav/             # Examiner navigation stack
â”‚   â”‚   â””â”€â”€ studentNav/              # Student navigation stack
â”‚   â”œâ”€â”€ screens/
â”‚   â”‚   â”œâ”€â”€ auth/
â”‚   â”‚   â”‚   â”œâ”€â”€ Login.js             # Login screen
â”‚   â”‚   â”‚   â””â”€â”€ Register.js          # Registration screen
â”‚   â”‚   â”œâ”€â”€ examiner/
â”‚   â”‚   â”‚   â”œâ”€â”€ ExaminerDashboard.js # Examiner main dashboard
â”‚   â”‚   â”‚   â”œâ”€â”€ CreateExam.js        # Exam creation interface
â”‚   â”‚   â”‚   â”œâ”€â”€ EditExam.js          # Exam editing
â”‚   â”‚   â”‚   â”œâ”€â”€ ManageExams.js       # Exam management
â”‚   â”‚   â”‚   â”œâ”€â”€ ExamSubmissions.js   # View submissions
â”‚   â”‚   â”‚   â””â”€â”€ ResearchAnalytics.js # Analytics dashboard
â”‚   â”‚   â””â”€â”€ student/
â”‚   â”‚       â”œâ”€â”€ HomePage.js          # Student home
â”‚   â”‚       â”œâ”€â”€ ExamPage.js          # Exam taking interface
â”‚   â”‚       â”œâ”€â”€ ViewResults.js       # Results viewing
â”‚   â”‚       â”œâ”€â”€ AccessibilitySettings.js # Accessibility config
â”‚   â”‚       â””â”€â”€ UsabilitySurvey.js   # Post-exam survey
â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”œâ”€â”€ examService.js           # Exam CRUD operations
â”‚   â”‚   â”œâ”€â”€ offlineService.js        # Offline sync logic
â”‚   â”‚   â”œâ”€â”€ speechService.js         # Speech recognition
â”‚   â”‚   â”œâ”€â”€ screenContextService.js  # Screen reader context
â”‚   â”‚   â””â”€â”€ evaluationMetrics.js    # Research analytics
â”‚   â””â”€â”€ utils/
â”‚       â””â”€â”€ grading.js               # Auto-grading logic
â”œâ”€â”€ android/                         # Android native configuration
â”œâ”€â”€ assets/                          # Images and icons
â”œâ”€â”€ App.js                           # Root component
â”œâ”€â”€ package.json                     # Dependencies
â””â”€â”€ app.json                         # Expo configuration
```

## ğŸ”§ Installation & Setup

### Prerequisites

- Node.js (v18 or higher)
- npm or yarn
- Expo CLI (`npm install -g expo-cli`)
- Firebase account (for backend services)
- Android Studio (for Android development) or Xcode (for iOS development)

### Installation Steps

1. **Clone the repository**

   ```bash
   git clone <repository-url>
   cd ExamAssistant
   ```

2. **Install dependencies**

   ```bash
   npm install
   ```

3. **Firebase Setup**

   - Create a Firebase project at [Firebase Console](https://console.firebase.google.com/)
   - Enable Authentication (Email/Password)
   - Create a Firestore database
   - Set up Firestore security rules (see `FIREBASE_SETUP.md`)
   - **Configure Environment Variables** (see Environment Configuration below)

4. **Run the application**

   ```bash
   # Start Expo development server
   npm start

   # Run on Android
   npm run android

   # Run on iOS
   npm run ios

   # Run on Web
   npm run web
   ```

### Environment Configuration

The app uses environment variables to securely store Firebase credentials. This prevents sensitive keys from being committed to git.

1. **Create `.env` file** in the root directory:

   ```bash
   cp .env.example .env
   ```

2. **Get your Firebase credentials** from [Firebase Console](https://console.firebase.google.com/):

   - Go to Project Settings > General
   - Scroll down to "Your apps" section
   - Copy the config values

3. **Fill in `.env` file** with your Firebase credentials:

   ```env
   FIREBASE_API_KEY=your-api-key-here
   FIREBASE_AUTH_DOMAIN=your-project.firebaseapp.com
   FIREBASE_PROJECT_ID=your-project-id
   FIREBASE_STORAGE_BUCKET=your-project.appspot.com
   FIREBASE_MESSAGING_SENDER_ID=your-sender-id
   FIREBASE_APP_ID=your-app-id
   FIREBASE_MEASUREMENT_ID=your-measurement-id
   ```

4. **Verify `.env` is in `.gitignore`** (it should be already):
   - The `.env` file is automatically ignored by git
   - Only `.env.example` is committed to the repository

> âš ï¸ **Important**: Never commit your `.env` file to git. It contains sensitive credentials. The `.env.example` file serves as a template for other developers.

## ğŸ¨ Key Technical Achievements

### 1. **Accessibility Architecture**

- Implemented comprehensive accessibility context with persistent settings
- Voice navigation system with speech recognition
- Haptic feedback for user interactions
- Screen reader compatibility

### 2. **Offline-First Design**

- Local caching of exams using AsyncStorage
- Automatic sync when connection is restored
- Queue system for pending submissions
- Network status detection without native modules

### 3. **Research Analytics System**

- Comprehensive evaluation metrics tracking
- Per-question interaction logging
- Voice command accuracy measurement
- SUS (System Usability Scale) survey integration
- Aggregated statistics for research analysis

### 4. **Role-Based Architecture**

- Secure role-based navigation
- Context-based authentication state management
- Separate navigation stacks for different user roles
- Firestore security rules for data protection

### 5. **Auto-Grading System**

- Automatic grading for multiple choice questions
- Written answer support with expected answer matching
- Real-time score calculation
- Results persistence in Firestore

## ğŸ“Š Database Schema

### Firestore Collections

**`users`** - User profiles

```javascript
{
  firstName: string,
  lastName: string,
  email: string,
  role: 'student' | 'examiner',
  name: string,
  createdAt: timestamp
}
```

**`exams`** - Exam documents

```javascript
{
  title: string,
  subject: string,
  duration: number,
  questions: Array<Question>,
  examinerId: string,
  status: 'draft' | 'active' | 'inactive',
  createdAt: timestamp,
  updatedAt: timestamp,
  totalQuestions: number
}
```

**`evaluationMetrics`** - Research analytics

```javascript
{
  examId: string,
  studentId: string,
  startTime: timestamp,
  endTime: timestamp,
  questionMetrics: Array,
  totalVoiceCommands: number,
  accessibilityFeaturesUsed: Object,
  status: string
}
```

**`usabilitySurveys`** - Post-exam surveys

```javascript
{
  sessionId: string,
  examId: string,
  studentId: string,
  responses: Object,
  susScore: number,
  completedAt: timestamp
}
```

## ğŸ” Security Features

- Firebase Authentication with email/password
- Firestore security rules for role-based access
- Secure local storage for offline data
- Input validation and error handling

## ğŸ“± Platform Support

- âœ… Android (fully supported)
- âœ… iOS (configured, requires testing)
- âœ… Web (basic support via Expo)

## ğŸ§ª Testing & Quality Assurance

- Error handling throughout the application
- Loading states for async operations
- Network error recovery
- Data validation on forms
- Offline mode testing

## ğŸš§ Future Enhancements

### Short-Term 

- [ ] **Multilingual Support**: Add TTS/STT support for multiple languages (Swahili, French, etc.)
- [ ] **Noise Handling**: Implement noise cancellation and voice enhancement algorithms
- [ ] **Command Confirmation**: Add "Did you mean..." prompts for ambiguous commands
- [ ] **Enhanced Offline Mode**: Pre-cache all exam data and improve sync reliability
- [ ] **Unit and Integration Tests**: Comprehensive test coverage for critical accessibility features

### Medium-Term 

- [ ] **Exam Integrity Protections**:
  - Screen lock during exams
  - Time limits enforcement
  - Anti-cheating measures
  - Exam session recording
- [ ] **Advanced Analytics**:
  - Real-time dashboards for examiners
  - Detailed performance metrics
  - Accessibility usage patterns
- [ ] **Push Notifications**: Alert students about new exams and examiners about submissions
- [ ] **Export Functionality**: Export analytics data and exam results in multiple formats

### Long-Term 

- [ ] **AI-Powered Voice Recognition**: Integration with advanced STT models for better accuracy
- [ ] **Adaptive Question Reading**: Adjust reading speed and style based on user preferences
- [ ] **Collaborative Features**: Group exams, peer review capabilities
- [ ] **Integration with LMS**: Connect with existing learning management systems
- [ ] **Screen Reader Optimization**: Deep integration with TalkBack and VoiceOver
- [ ] **Exam Scheduling System**: Automated scheduling with calendar integration

### Research Focus

- [ ] **Usability Studies**: Conduct formal accessibility research with visually impaired students
- [ ] **Performance Metrics**: Publish findings on voice-driven exam effectiveness
- [ ] **Comparative Analysis**: Compare independent exam-taking vs. scribe-assisted methods

**Note**: This section demonstrates forward-thinking and awareness of production requirements, showing maturity beyond "it works."

## ğŸ“š Documentation

- `FIREBASE_SETUP.md` - Firebase configuration guide
- `NAVIGATION.md` - Navigation structure documentation
- `FIRESTORE_INDEXES.md` - Database indexing requirements

## ğŸ‘¨â€ğŸ’» Development

### Code Style

- React Native best practices
- Functional components with hooks
- Context API for state management
- Service layer pattern for business logic

### Key Design Patterns

- **Provider Pattern**: Context providers for global state
- **Service Layer**: Separation of business logic from UI
- **Repository Pattern**: Centralized data access through services
- **Observer Pattern**: Real-time updates from Firestore

## ğŸ“„ License

This is a public research project for accessibility evaluation. The codebase is open for educational and research purposes.

## ğŸ¤ Contributing

This is a research project for accessibility evaluation. For contributions or questions, please contact the project maintainer.

## ğŸ“ Contact

**Developer**: Powell Mweemba 
**Project**: ExamAssistant - Smart AI Exam System  
**Purpose**: Accessibility research for visually impaired students

---

### Project Impact

- Addresses real-world accessibility challenges in education
- Enables independent exam-taking for visually impaired students
- Provides research data for improving accessibility features
- Demonstrates full-stack mobile development capabilities

---

**Built with â¤ï¸ for accessibility and education**
