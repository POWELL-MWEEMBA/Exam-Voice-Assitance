ExamAssistant â€“ Voice-Driven Exam System for Visually Impaired Users

A React Native (Expo) mobile application enabling visually impaired students to take exams independently using voice commands, offline support, and accessibility-first design.

ðŸ“± Live APK (Android):
ðŸ‘‰ https://expo.dev/artifacts/eas/fHx7RRmHDj7PfsFH9QppaF.apk


Why This Project Matters

Traditional exam systems rely on visual interfaces, forcing visually impaired students to depend on human scribes or external tools.
ExamAssistant enables fully independent, voice-only exam taking.

Core Capabilities 

ðŸŽ™ Voice-driven navigation (TTS + STT)

ðŸ”’ Strict TTS/STT state locking (no feedback loops)

ðŸ“¡ Offline-first exam taking with auto-sync

ðŸ‘¥ Role-based access (Student / Examiner)

ðŸ“Š Research analytics & usability metrics

â™¿ Accessibility-first UX (voice, haptics, no visual dependency)


Technical Highlights 

React Native + Expo (Android)

Firebase Auth + Firestore

Offline caching with AsyncStorage

Explicit TTS/STT state machine

Context-based command validation

AI-assisted development with full ownership

Architecture Snapshot

The system enforces a turn-based voice interaction model where Text-to-Speech and Speech-to-Text are mutually exclusive, preventing feedback loops and ensuring reliable voice control.

Note: For more info read the #READ-DETAILED.md
